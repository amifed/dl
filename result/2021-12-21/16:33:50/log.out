dataset_path: /home/djy/dataset/dataset2
pretrained : False 
parallel: True

parallel segmentent dataset : /home/djy/dataset/ycrcb_hsv_dataset2
msg: cbam resnet34
using model: ResNet, resnet34
using device cuda:0
batch_size = 18
epochs = 64
loss_function = CrossEntropyLoss()
optimizer = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)


========== Train Epoch 1 ==========
Loss: 1.857	Accuracy: 32.43%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.5000    0.1290    0.2051        31
         cwx     0.2174    0.2273    0.2222        22
         hdx     0.0000    0.0000    0.0000        17
         mtx     0.6000    0.1579    0.2500        19
         nqx     0.1290    0.3333    0.1860        12
         qtx     0.3750    0.4000    0.3871        45
         zxx     0.3714    0.6667    0.4771        39

    accuracy                         0.3243       185
   macro avg     0.3133    0.2735    0.2468       185
weighted avg     0.3491    0.3243    0.2933       185

micro f-score: 0.32432432432432434

========== Train Epoch 2 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.615	Accuracy: 40.54%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.3438    0.3548    0.3492        31
         cwx     0.3333    0.2273    0.2703        22
         hdx     0.1765    0.1765    0.1765        17
         mtx     0.0000    0.0000    0.0000        19
         nqx     0.3846    0.8333    0.5263        12
         qtx     0.5122    0.4667    0.4884        45
         zxx     0.4630    0.6410    0.5376        39

    accuracy                         0.4054       185
   macro avg     0.3162    0.3857    0.3355       185
weighted avg     0.3606    0.4054    0.3731       185

micro f-score: 0.40540540540540543

========== Train Epoch 3 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.409	Accuracy: 41.08%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.2632    0.8065    0.3968        31
         cwx     0.3182    0.3182    0.3182        22
         hdx     0.0000    0.0000    0.0000        17
         mtx     0.5714    0.2105    0.3077        19
         nqx     0.3704    0.8333    0.5128        12
         qtx     0.9048    0.4222    0.5758        45
         zxx     0.8462    0.2821    0.4231        39

    accuracy                         0.4108       185
   macro avg     0.4677    0.4104    0.3621       185
weighted avg     0.5631    0.4108    0.3984       185

micro f-score: 0.4108108108108109

========== Train Epoch 4 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.181	Accuracy: 45.41%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4500    0.2903    0.3529        31
         cwx     0.5385    0.3182    0.4000        22
         hdx     0.0000    0.0000    0.0000        17
         mtx     0.2222    0.1053    0.1429        19
         nqx     0.2439    0.8333    0.3774        12
         qtx     0.8214    0.5111    0.6301        45
         zxx     0.4459    0.8462    0.5841        39

    accuracy                         0.4541       185
   macro avg     0.3889    0.4149    0.3553       185
weighted avg     0.4719    0.4541    0.4223       185

micro f-score: 0.4540540540540541

========== Train Epoch 5 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.053	Accuracy: 32.97%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.2500    0.2273    0.2381        22
         hdx     0.0000    0.0000    0.0000        17
         mtx     0.0000    0.0000    0.0000        19
         nqx     1.0000    0.0833    0.1538        12
         qtx     0.8947    0.3778    0.5312        45
         zxx     0.2621    0.9744    0.4130        39

    accuracy                         0.3297       185
   macro avg     0.3438    0.2375    0.1909       185
weighted avg     0.3675    0.3297    0.2546       185

micro f-score: 0.32972972972972975

========== Train Epoch 6 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.782	Accuracy: 47.03%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.3396    0.5806    0.4286        31
         cwx     0.2951    0.8182    0.4337        22
         hdx     0.5000    0.2353    0.3200        17
         mtx     1.0000    0.1053    0.1905        19
         nqx     0.5000    0.5000    0.5000        12
         qtx     0.7143    0.5556    0.6250        45
         zxx     1.0000    0.3590    0.5283        39

    accuracy                         0.4703       185
   macro avg     0.6213    0.4506    0.4323       185
weighted avg     0.6576    0.4703    0.4682       185

micro f-score: 0.4702702702702703

========== Train Epoch 7 ==========
Loss: 0.630	Accuracy: 38.38%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3158    0.3871    0.3478        31
         cwx     0.2419    0.6818    0.3571        22
         hdx     0.3158    0.3529    0.3333        17
         mtx     0.3333    0.0526    0.0909        19
         nqx     0.3684    0.5833    0.4516        12
         qtx     1.0000    0.2444    0.3929        45
         zxx     0.5758    0.4872    0.5278        39

    accuracy                         0.3838       185
   macro avg     0.4501    0.3985    0.3574       185
weighted avg     0.5335    0.3838    0.3768       185

micro f-score: 0.3837837837837838

========== Train Epoch 8 ==========
Loss: 0.341	Accuracy: 50.27%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.3667    0.7097    0.4835        31
         cwx     1.0000    0.2727    0.4286        22
         hdx     0.2121    0.4118    0.2800        17
         mtx     0.3158    0.3158    0.3158        19
         nqx     0.7500    0.5000    0.6000        12
         qtx     0.8276    0.5333    0.6486        45
         zxx     0.7333    0.5641    0.6377        39

    accuracy                         0.5027       185
   macro avg     0.6008    0.4725    0.4849       185
weighted avg     0.6368    0.5027    0.5213       185

micro f-score: 0.5027027027027027

========== Train Epoch 9 ==========
Loss: 0.253	Accuracy: 45.95%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4118    0.2258    0.2917        31
         cwx     0.3889    0.6364    0.4828        22
         hdx     0.2273    0.5882    0.3279        17
         mtx     0.2692    0.3684    0.3111        19
         nqx     0.0000    0.0000    0.0000        12
         qtx     0.8182    0.4000    0.5373        45
         zxx     0.7250    0.7436    0.7342        39

    accuracy                         0.4595       185
   macro avg     0.4058    0.4232    0.3836       185
weighted avg     0.5156    0.4595    0.4538       185

micro f-score: 0.4594594594594595

========== Train Epoch 10 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.179	Accuracy: 51.35%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5556    0.3226    0.4082        31
         cwx     0.4815    0.5909    0.5306        22
         hdx     0.2500    0.4118    0.3111        17
         mtx     0.3000    0.1579    0.2069        19
         nqx     0.3846    0.4167    0.4000        12
         qtx     0.8800    0.4889    0.6286        45
         zxx     0.5469    0.8974    0.6796        39

    accuracy                         0.5135       185
   macro avg     0.4855    0.4694    0.4521       185
weighted avg     0.5584    0.5135    0.5034       185

micro f-score: 0.5135135135135135

========== Train Epoch 11 ==========
Loss: 0.114	Accuracy: 35.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.8000    0.1290    0.2222        31
         cwx     0.1909    0.9545    0.3182        22
         hdx     1.0000    0.0588    0.1111        17
         mtx     0.4000    0.1053    0.1667        19
         nqx     0.2500    0.0833    0.1250        12
         qtx     0.9000    0.2000    0.3273        45
         zxx     0.5400    0.6923    0.6067        39

    accuracy                         0.3514       185
   macro avg     0.5830    0.3176    0.2682       185
weighted avg     0.6387    0.3514    0.3180       185

micro f-score: 0.35135135135135137

========== Train Epoch 12 ==========
Loss: 0.090	Accuracy: 54.05%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.5263    0.3226    0.4000        31
         cwx     0.5652    0.5909    0.5778        22
         hdx     0.3636    0.2353    0.2857        17
         mtx     0.4211    0.4211    0.4211        19
         nqx     0.3333    0.7500    0.4615        12
         qtx     0.8214    0.5111    0.6301        45
         zxx     0.5690    0.8462    0.6804        39

    accuracy                         0.5405       185
   macro avg     0.5143    0.5253    0.4938       185
weighted avg     0.5734    0.5405    0.5319       185

micro f-score: 0.5405405405405406

========== Train Epoch 13 ==========
Loss: 0.071	Accuracy: 49.73%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5652    0.4194    0.4815        31
         cwx     0.5789    0.5000    0.5366        22
         hdx     0.2059    0.4118    0.2745        17
         mtx     0.2500    0.2105    0.2286        19
         nqx     0.3684    0.5833    0.4516        12
         qtx     0.5833    0.6222    0.6022        45
         zxx     0.8462    0.5641    0.6769        39

    accuracy                         0.4973       185
   macro avg     0.4854    0.4730    0.4645       185
weighted avg     0.5523    0.4973    0.5117       185

micro f-score: 0.4972972972972973

========== Train Epoch 14 ==========
Loss: 0.056	Accuracy: 50.81%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5556    0.1613    0.2500        31
         cwx     0.5882    0.4545    0.5128        22
         hdx     0.5714    0.2353    0.3333        17
         mtx     0.2500    0.5789    0.3492        19
         nqx     0.4444    0.6667    0.5333        12
         qtx     0.7742    0.5333    0.6316        45
         zxx     0.5424    0.8205    0.6531        39

    accuracy                         0.5081       185
   macro avg     0.5323    0.4929    0.4662       185
weighted avg     0.5727    0.5081    0.4953       185

micro f-score: 0.5081081081081081

========== Train Epoch 15 ==========
Loss: 0.059	Accuracy: 41.08%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.2393    0.9032    0.3784        31
         cwx     0.7500    0.1364    0.2308        22
         hdx     0.4545    0.2941    0.3571        17
         mtx     0.5556    0.2632    0.3571        19
         nqx     0.7500    0.2500    0.3750        12
         qtx     0.7647    0.5778    0.6582        45
         zxx     1.0000    0.1538    0.2667        39

    accuracy                         0.4108       185
   macro avg     0.6449    0.3684    0.3748       185
weighted avg     0.6736    0.4108    0.4010       185

micro f-score: 0.4108108108108109

========== Train Epoch 16 ==========
Loss: 0.091	Accuracy: 42.16%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.2927    0.7742    0.4248        31
         cwx     0.5556    0.2273    0.3226        22
         hdx     0.2000    0.1176    0.1481        17
         mtx     0.2000    0.4211    0.2712        19
         nqx     0.7500    0.2500    0.3750        12
         qtx     0.8333    0.4444    0.5797        45
         zxx     1.0000    0.4103    0.5818        39

    accuracy                         0.4216       185
   macro avg     0.5474    0.3778    0.3862       185
weighted avg     0.6162    0.4216    0.4390       185

micro f-score: 0.42162162162162165

========== Train Epoch 17 ==========
Loss: 0.123	Accuracy: 47.03%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4324    0.5161    0.4706        31
         cwx     0.5556    0.2273    0.3226        22
         hdx     0.1970    0.7647    0.3133        17
         mtx     0.5000    0.0526    0.0952        19
         nqx     0.3333    0.0833    0.1333        12
         qtx     0.7568    0.6222    0.6829        45
         zxx     0.7419    0.5897    0.6571        39

    accuracy                         0.4703       185
   macro avg     0.5024    0.4080    0.3822       185
weighted avg     0.5701    0.4703    0.4691       185

micro f-score: 0.4702702702702703

========== Train Epoch 18 ==========
Loss: 0.076	Accuracy: 48.65%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4062    0.4194    0.4127        31
         cwx     0.4667    0.3182    0.3784        22
         hdx     0.4444    0.2353    0.3077        17
         mtx     0.2500    0.3158    0.2791        19
         nqx     0.3333    0.5833    0.4242        12
         qtx     0.6429    0.6000    0.6207        45
         zxx     0.6190    0.6667    0.6420        39

    accuracy                         0.4865       185
   macro avg     0.4518    0.4484    0.4378       185
weighted avg     0.4986    0.4865    0.4849       185

micro f-score: 0.4864864864864865

========== Train Epoch 19 ==========
Loss: 0.077	Accuracy: 50.27%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.6000    0.3871    0.4706        31
         cwx     0.3846    0.4545    0.4167        22
         hdx     0.2500    0.4118    0.3111        17
         mtx     0.2593    0.3684    0.3043        19
         nqx     0.6000    0.2500    0.3529        12
         qtx     0.7568    0.6222    0.6829        45
         zxx     0.6190    0.6667    0.6420        39

    accuracy                         0.5027       185
   macro avg     0.4957    0.4515    0.4544       185
weighted avg     0.5494    0.5027    0.5126       185

micro f-score: 0.5027027027027027

========== Train Epoch 20 ==========
Loss: 0.063	Accuracy: 51.35%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4091    0.5806    0.4800        31
         cwx     0.5333    0.3636    0.4324        22
         hdx     0.7500    0.1765    0.2857        17
         mtx     0.3333    0.1053    0.1600        19
         nqx     0.0000    0.0000    0.0000        12
         qtx     0.5000    0.7556    0.6018        45
         zxx     0.6250    0.7692    0.6897        39

    accuracy                         0.5135       185
   macro avg     0.4501    0.3930    0.3785       185
weighted avg     0.4885    0.5135    0.4663       185

micro f-score: 0.5135135135135135

========== Train Epoch 21 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.087	Accuracy: 43.24%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3529    0.1935    0.2500        31
         cwx     0.6667    0.2727    0.3871        22
         hdx     0.3889    0.4118    0.4000        17
         mtx     0.5000    0.2632    0.3448        19
         nqx     1.0000    0.0833    0.1538        12
         qtx     0.8000    0.3556    0.4923        45
         zxx     0.3545    1.0000    0.5235        39

    accuracy                         0.4324       185
   macro avg     0.5804    0.3686    0.3645       185
weighted avg     0.5597    0.4324    0.4002       185

micro f-score: 0.43243243243243246

========== Train Epoch 22 ==========
Loss: 0.068	Accuracy: 50.81%	Cost 36s
              precision    recall  f1-score   support

         bzx     1.0000    0.0645    0.1212        31
         cwx     0.4103    0.7273    0.5246        22
         hdx     0.3684    0.4118    0.3889        17
         mtx     0.3000    0.1579    0.2069        19
         nqx     0.3684    0.5833    0.4516        12
         qtx     0.5593    0.7333    0.6346        45
         zxx     0.7027    0.6667    0.6842        39

    accuracy                         0.5081       185
   macro avg     0.5299    0.4778    0.4303       185
weighted avg     0.5891    0.5081    0.4676       185

micro f-score: 0.5081081081081081

========== Train Epoch 23 ==========
Loss: 0.045	Accuracy: 49.19%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4118    0.2258    0.2917        31
         cwx     0.3208    0.7727    0.4533        22
         hdx     0.2222    0.2353    0.2286        17
         mtx     0.3571    0.2632    0.3030        19
         nqx     0.6667    0.6667    0.6667        12
         qtx     0.6944    0.5556    0.6173        45
         zxx     0.7143    0.6410    0.6757        39

    accuracy                         0.4919       185
   macro avg     0.4839    0.4800    0.4623       185
weighted avg     0.5270    0.4919    0.4907       185

micro f-score: 0.4918918918918919

========== Train Epoch 24 ==========
Loss: 0.042	Accuracy: 49.19%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3750    0.0968    0.1538        31
         cwx     0.8333    0.2273    0.3571        22
         hdx     0.5000    0.0588    0.1053        17
         mtx     0.3889    0.3684    0.3784        19
         nqx     0.2778    0.8333    0.4167        12
         qtx     0.7073    0.6444    0.6744        45
         zxx     0.4865    0.9231    0.6372        39

    accuracy                         0.4919       185
   macro avg     0.5098    0.4503    0.3890       185
weighted avg     0.5404    0.4919    0.4422       185

micro f-score: 0.4918918918918919

========== Train Epoch 25 ==========
Loss: 0.042	Accuracy: 44.86%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.6364    0.2258    0.3333        31
         cwx     0.5263    0.4545    0.4878        22
         hdx     0.1837    0.5294    0.2727        17
         mtx     0.1667    0.2632    0.2041        19
         nqx     1.0000    0.3333    0.5000        12
         qtx     0.8696    0.4444    0.5882        45
         zxx     0.5714    0.7179    0.6364        39

    accuracy                         0.4486       185
   macro avg     0.5649    0.4241    0.4318       185
weighted avg     0.6001    0.4486    0.4696       185

micro f-score: 0.4486486486486486

========== Train Epoch 26 ==========
Loss: 0.041	Accuracy: 52.97%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4872    0.6129    0.5429        31
         cwx     0.5200    0.5909    0.5532        22
         hdx     0.3333    0.2353    0.2759        17
         mtx     0.3333    0.0526    0.0909        19
         nqx     0.4706    0.6667    0.5517        12
         qtx     0.5385    0.6222    0.5773        45
         zxx     0.6757    0.6410    0.6579        39

    accuracy                         0.5297       185
   macro avg     0.4798    0.4888    0.4643       185
weighted avg     0.5123    0.5297    0.5063       185

micro f-score: 0.5297297297297298

========== Train Epoch 27 ==========
Loss: 0.030	Accuracy: 55.68%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.6667    0.2581    0.3721        31
         cwx     0.4800    0.5455    0.5106        22
         hdx     0.5000    0.1765    0.2609        17
         mtx     0.5000    0.2105    0.2963        19
         nqx     0.6000    0.5000    0.5455        12
         qtx     0.7442    0.7111    0.7273        45
         zxx     0.4691    0.9744    0.6333        39

    accuracy                         0.5568       185
   macro avg     0.5657    0.4823    0.4780       185
weighted avg     0.5849    0.5568    0.5233       185

micro f-score: 0.5567567567567567

========== Train Epoch 28 ==========
Loss: 0.020	Accuracy: 52.43%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4286    0.2903    0.3462        31
         cwx     0.4583    0.5000    0.4783        22
         hdx     0.5000    0.3529    0.4138        17
         mtx     0.4286    0.1579    0.2308        19
         nqx     0.3500    0.5833    0.4375        12
         qtx     0.7179    0.6222    0.6667        45
         zxx     0.5323    0.8462    0.6535        39

    accuracy                         0.5243       185
   macro avg     0.4880    0.4790    0.4609       185
weighted avg     0.5258    0.5243    0.5049       185

micro f-score: 0.5243243243243243

========== Train Epoch 29 ==========
Loss: 0.024	Accuracy: 55.14%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.6667    0.2581    0.3721        31
         cwx     0.5000    0.5909    0.5417        22
         hdx     0.4286    0.1765    0.2500        17
         mtx     0.3333    0.1579    0.2143        19
         nqx     0.3889    0.5833    0.4667        12
         qtx     0.7174    0.7333    0.7253        45
         zxx     0.5224    0.8974    0.6604        39

    accuracy                         0.5514       185
   macro avg     0.5082    0.4853    0.4615       185
weighted avg     0.5546    0.5514    0.5176       185

micro f-score: 0.5513513513513514

========== Train Epoch 30 ==========
Loss: 0.016	Accuracy: 56.22%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5385    0.4516    0.4912        31
         cwx     0.5000    0.5455    0.5217        22
         hdx     0.3333    0.4706    0.3902        17
         mtx     0.2500    0.0526    0.0870        19
         nqx     0.4211    0.6667    0.5161        12
         qtx     0.7500    0.6667    0.7059        45
         zxx     0.6458    0.7949    0.7126        39

    accuracy                         0.5622       185
   macro avg     0.4912    0.5212    0.4893       185
weighted avg     0.5519    0.5622    0.5446       185

micro f-score: 0.5621621621621622

========== Train Epoch 31 ==========
Loss: 0.018	Accuracy: 57.84%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4651    0.6452    0.5405        31
         cwx     0.5600    0.6364    0.5957        22
         hdx     0.4444    0.2353    0.3077        17
         mtx     0.4615    0.3158    0.3750        19
         nqx     0.5294    0.7500    0.6207        12
         qtx     0.8519    0.5111    0.6389        45
         zxx     0.6078    0.7949    0.6889        39

    accuracy                         0.5784       185
   macro avg     0.5600    0.5555    0.5382       185
weighted avg     0.6025    0.5784    0.5691       185

micro f-score: 0.5783783783783784

========== Train Epoch 32 ==========
Loss: 0.029	Accuracy: 54.59%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5714    0.2581    0.3556        31
         cwx     0.4667    0.6364    0.5385        22
         hdx     0.7143    0.2941    0.4167        17
         mtx     0.5000    0.1053    0.1739        19
         nqx     0.4667    0.5833    0.5185        12
         qtx     0.8000    0.6222    0.7000        45
         zxx     0.4625    0.9487    0.6218        39

    accuracy                         0.5459       185
   macro avg     0.5688    0.4926    0.4750       185
weighted avg     0.5906    0.5459    0.5148       185

micro f-score: 0.5459459459459459

========== Train Epoch 33 ==========
Loss: 0.019	Accuracy: 57.30%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.5500    0.3548    0.4314        31
         cwx     0.5000    0.5000    0.5000        22
         hdx     0.5455    0.3529    0.4286        17
         mtx     0.4545    0.2632    0.3333        19
         nqx     0.5263    0.8333    0.6452        12
         qtx     0.7692    0.6667    0.7143        45
         zxx     0.5238    0.8462    0.6471        39

    accuracy                         0.5730       185
   macro avg     0.5528    0.5453    0.5285       185
weighted avg     0.5801    0.5730    0.5574       185

micro f-score: 0.572972972972973

========== Train Epoch 34 ==========
Loss: 0.016	Accuracy: 60.00%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4545    0.4839    0.4687        31
         cwx     0.6316    0.5455    0.5854        22
         hdx     0.6250    0.2941    0.4000        17
         mtx     0.3750    0.3158    0.3429        19
         nqx     0.7778    0.5833    0.6667        12
         qtx     0.6604    0.7778    0.7143        45
         zxx     0.6596    0.7949    0.7209        39

    accuracy                         0.6000       185
   macro avg     0.5977    0.5422    0.5570       185
weighted avg     0.5973    0.6000    0.5891       185

micro f-score: 0.6

========== Train Epoch 35 ==========
Loss: 0.021	Accuracy: 55.68%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4706    0.5161    0.4923        31
         cwx     0.4348    0.4545    0.4444        22
         hdx     0.4000    0.2353    0.2963        17
         mtx     0.5000    0.1053    0.1739        19
         nqx     0.3846    0.8333    0.5263        12
         qtx     0.6522    0.6667    0.6593        45
         zxx     0.7381    0.7949    0.7654        39

    accuracy                         0.5568       185
   macro avg     0.5115    0.5152    0.4797       185
weighted avg     0.5579    0.5568    0.5363       185

micro f-score: 0.5567567567567567

========== Train Epoch 36 ==========
Loss: 0.026	Accuracy: 54.59%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.6154    0.2581    0.3636        31
         cwx     0.4545    0.4545    0.4545        22
         hdx     0.4615    0.3529    0.4000        17
         mtx     0.3636    0.2105    0.2667        19
         nqx     0.4000    0.5000    0.4444        12
         qtx     0.6275    0.7111    0.6667        45
         zxx     0.5833    0.8974    0.7071        39

    accuracy                         0.5459       185
   macro avg     0.5008    0.4835    0.4719       185
weighted avg     0.5385    0.5459    0.5192       185

micro f-score: 0.5459459459459459

========== Train Epoch 37 ==========
Loss: 0.020	Accuracy: 55.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4571    0.5161    0.4848        31
         cwx     0.7143    0.2273    0.3448        22
         hdx     0.4074    0.6471    0.5000        17
         mtx     0.2500    0.3684    0.2979        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.8108    0.6667    0.7317        45
         zxx     0.6829    0.7179    0.7000        39

    accuracy                         0.5514       185
   macro avg     0.5461    0.5086    0.5020       185
weighted avg     0.5983    0.5514    0.5538       185

micro f-score: 0.5513513513513514

========== Train Epoch 38 ==========
Loss: 0.017	Accuracy: 55.14%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5500    0.3548    0.4314        31
         cwx     0.4211    0.7273    0.5333        22
         hdx     0.7500    0.1765    0.2857        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.4000    0.5000    0.4444        12
         qtx     0.6591    0.6444    0.6517        45
         zxx     0.6071    0.8718    0.7158        39

    accuracy                         0.5514       185
   macro avg     0.5375    0.4904    0.4692       185
weighted avg     0.5639    0.5514    0.5230       185

micro f-score: 0.5513513513513514

========== Train Epoch 39 ==========
Loss: 0.012	Accuracy: 57.30%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4865    0.5806    0.5294        31
         cwx     0.4516    0.6364    0.5283        22
         hdx     0.5714    0.4706    0.5161        17
         mtx     0.3333    0.1053    0.1600        19
         nqx     0.4444    0.6667    0.5333        12
         qtx     0.7179    0.6222    0.6667        45
         zxx     0.7000    0.7179    0.7089        39

    accuracy                         0.5730       185
   macro avg     0.5293    0.5428    0.5204       185
weighted avg     0.5730    0.5730    0.5616       185

micro f-score: 0.572972972972973

========== Train Epoch 40 ==========
Loss: 0.014	Accuracy: 59.46%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5357    0.4839    0.5085        31
         cwx     0.4688    0.6818    0.5556        22
         hdx     0.5333    0.4706    0.5000        17
         mtx     0.3846    0.2632    0.3125        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.8182    0.6000    0.6923        45
         zxx     0.6667    0.8718    0.7556        39

    accuracy                         0.5946       185
   macro avg     0.5527    0.5530    0.5435       185
weighted avg     0.6035    0.5946    0.5881       185

micro f-score: 0.5945945945945946

========== Train Epoch 41 ==========
Loss: 0.013	Accuracy: 57.30%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5217    0.3871    0.4444        31
         cwx     0.5556    0.4545    0.5000        22
         hdx     0.4000    0.4706    0.4324        17
         mtx     0.2857    0.1053    0.1538        19
         nqx     0.6250    0.8333    0.7143        12
         qtx     0.6531    0.7111    0.6809        45
         zxx     0.6154    0.8205    0.7033        39

    accuracy                         0.5730       185
   macro avg     0.5224    0.5404    0.5185       185
weighted avg     0.5487    0.5730    0.5497       185

micro f-score: 0.572972972972973

========== Train Epoch 42 ==========
Loss: 0.013	Accuracy: 57.84%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.4615    0.5806    0.5143        31
         cwx     0.4444    0.5455    0.4898        22
         hdx     0.6250    0.2941    0.4000        17
         mtx     0.4286    0.1579    0.2308        19
         nqx     0.5294    0.7500    0.6207        12
         qtx     0.6471    0.7333    0.6875        45
         zxx     0.7500    0.6923    0.7200        39

    accuracy                         0.5784       185
   macro avg     0.5551    0.5363    0.5233       185
weighted avg     0.5815    0.5784    0.5642       185

micro f-score: 0.5783783783783784

========== Train Epoch 43 ==========
Loss: 0.011	Accuracy: 59.46%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5385    0.4516    0.4912        31
         cwx     0.5000    0.6364    0.5600        22
         hdx     0.5556    0.5882    0.5714        17
         mtx     0.2500    0.1579    0.1935        19
         nqx     0.5455    0.5000    0.5217        12
         qtx     0.7561    0.6889    0.7209        45
         zxx     0.6531    0.8205    0.7273        39

    accuracy                         0.5946       185
   macro avg     0.5427    0.5491    0.5409       185
weighted avg     0.5834    0.5946    0.5838       185

micro f-score: 0.5945945945945946

========== Train Epoch 44 ==========
Loss: 0.011	Accuracy: 58.38%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5200    0.4194    0.4643        31
         cwx     0.4483    0.5909    0.5098        22
         hdx     0.4737    0.5294    0.5000        17
         mtx     0.2727    0.1579    0.2000        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.7561    0.6889    0.7209        45
         zxx     0.6809    0.8205    0.7442        39

    accuracy                         0.5838       185
   macro avg     0.5272    0.5415    0.5285       185
weighted avg     0.5744    0.5838    0.5735       185

micro f-score: 0.5837837837837838

========== Train Epoch 45 ==========
Loss: 0.011	Accuracy: 57.84%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5000    0.4194    0.4561        31
         cwx     0.4516    0.6364    0.5283        22
         hdx     0.6364    0.4118    0.5000        17
         mtx     0.3333    0.1579    0.2143        19
         nqx     0.4737    0.7500    0.5806        12
         qtx     0.7317    0.6667    0.6977        45
         zxx     0.6458    0.7949    0.7126        39

    accuracy                         0.5784       185
   macro avg     0.5389    0.5481    0.5271       185
weighted avg     0.5751    0.5784    0.5648       185

micro f-score: 0.5783783783783784

========== Train Epoch 46 ==========
Loss: 0.013	Accuracy: 57.30%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5000    0.4839    0.4918        31
         cwx     0.5200    0.5909    0.5532        22
         hdx     0.5385    0.4118    0.4667        17
         mtx     0.6000    0.1579    0.2500        19
         nqx     0.3750    0.7500    0.5000        12
         qtx     0.7250    0.6444    0.6824        45
         zxx     0.6250    0.7692    0.6897        39

    accuracy                         0.5730       185
   macro avg     0.5548    0.5440    0.5191       185
weighted avg     0.5892    0.5730    0.5606       185

micro f-score: 0.572972972972973

========== Train Epoch 47 ==========
Loss: 0.014	Accuracy: 56.76%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5714    0.5161    0.5424        31
         cwx     0.4286    0.5455    0.4800        22
         hdx     0.3529    0.3529    0.3529        17
         mtx     0.5000    0.1579    0.2400        19
         nqx     0.4375    0.5833    0.5000        12
         qtx     0.6977    0.6667    0.6818        45
         zxx     0.6596    0.7949    0.7209        39

    accuracy                         0.5676       185
   macro avg     0.5211    0.5168    0.5026       185
weighted avg     0.5676    0.5676    0.5553       185

micro f-score: 0.5675675675675675

========== Train Epoch 48 ==========
Loss: 0.014	Accuracy: 55.68%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5417    0.4194    0.4727        31
         cwx     0.4324    0.7273    0.5424        22
         hdx     0.4000    0.3529    0.3750        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.3684    0.5833    0.4516        12
         qtx     0.7714    0.6000    0.6750        45
         zxx     0.6596    0.7949    0.7209        39

    accuracy                         0.5568       185
   macro avg     0.5069    0.5194    0.4943       185
weighted avg     0.5680    0.5568    0.5465       185

micro f-score: 0.5567567567567567

========== Train Epoch 49 ==========
Loss: 0.010	Accuracy: 57.30%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5556    0.4839    0.5172        31
         cwx     0.4839    0.6818    0.5660        22
         hdx     0.4167    0.2941    0.3448        17
         mtx     0.3333    0.1579    0.2143        19
         nqx     0.3889    0.5833    0.4667        12
         qtx     0.7368    0.6222    0.6747        45
         zxx     0.6600    0.8462    0.7416        39

    accuracy                         0.5730       185
   macro avg     0.5107    0.5242    0.5036       185
weighted avg     0.5667    0.5730    0.5584       185

micro f-score: 0.572972972972973

========== Train Epoch 50 ==========
Loss: 0.010	Accuracy: 57.30%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5161    0.5161    0.5161        31
         cwx     0.4815    0.5909    0.5306        22
         hdx     0.6667    0.3529    0.4615        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.4444    0.6667    0.5333        12
         qtx     0.6829    0.6222    0.6512        45
         zxx     0.6275    0.8205    0.7111        39

    accuracy                         0.5730       185
   macro avg     0.5420    0.5325    0.5180       185
weighted avg     0.5707    0.5730    0.5577       185

micro f-score: 0.572972972972973

========== Train Epoch 51 ==========
Loss: 0.011	Accuracy: 57.84%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5000    0.5806    0.5373        31
         cwx     0.4667    0.6364    0.5385        22
         hdx     0.4706    0.4706    0.4706        17
         mtx     0.4286    0.1579    0.2308        19
         nqx     0.5333    0.6667    0.5926        12
         qtx     0.7179    0.6222    0.6667        45
         zxx     0.6829    0.7179    0.7000        39

    accuracy                         0.5784       185
   macro avg     0.5429    0.5503    0.5338       185
weighted avg     0.5797    0.5784    0.5692       185

micro f-score: 0.5783783783783784

========== Train Epoch 52 ==========
Loss: 0.010	Accuracy: 55.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4839    0.4839    0.4839        31
         cwx     0.4375    0.6364    0.5185        22
         hdx     0.4286    0.3529    0.3871        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.5000    0.5833    0.5385        12
         qtx     0.6829    0.6222    0.6512        45
         zxx     0.6444    0.7436    0.6905        39

    accuracy                         0.5514       185
   macro avg     0.5075    0.5115    0.4988       185
weighted avg     0.5454    0.5514    0.5400       185

micro f-score: 0.5513513513513514

========== Train Epoch 53 ==========
Loss: 0.012	Accuracy: 57.84%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5769    0.4839    0.5263        31
         cwx     0.4545    0.6818    0.5455        22
         hdx     0.4667    0.4118    0.4375        17
         mtx     0.5000    0.1053    0.1739        19
         nqx     0.4706    0.6667    0.5517        12
         qtx     0.6182    0.7556    0.6800        45
         zxx     0.7429    0.6667    0.7027        39

    accuracy                         0.5784       185
   macro avg     0.5471    0.5388    0.5168       185
weighted avg     0.5825    0.5784    0.5605       185

micro f-score: 0.5783783783783784

========== Train Epoch 54 ==========
Loss: 0.011	Accuracy: 56.22%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5357    0.4839    0.5085        31
         cwx     0.4062    0.5909    0.4815        22
         hdx     0.5000    0.2941    0.3704        17
         mtx     0.5714    0.2105    0.3077        19
         nqx     0.4000    0.6667    0.5000        12
         qtx     0.7027    0.5778    0.6341        45
         zxx     0.6471    0.8462    0.7333        39

    accuracy                         0.5622       185
   macro avg     0.5376    0.5243    0.5051       185
weighted avg     0.5760    0.5622    0.5494       185

micro f-score: 0.5621621621621622

========== Train Epoch 55 ==========
Loss: 0.010	Accuracy: 59.46%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5556    0.4839    0.5172        31
         cwx     0.5000    0.6364    0.5600        22
         hdx     0.4286    0.5294    0.4737        17
         mtx     0.4286    0.1579    0.2308        19
         nqx     0.4545    0.4167    0.4348        12
         qtx     0.6957    0.7111    0.7033        45
         zxx     0.7111    0.8205    0.7619        39

    accuracy                         0.5946       185
   macro avg     0.5391    0.5365    0.5260       185
weighted avg     0.5846    0.5946    0.5804       185

micro f-score: 0.5945945945945946

========== Train Epoch 56 ==========
Loss: 0.012	Accuracy: 55.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4667    0.4516    0.4590        31
         cwx     0.4333    0.5909    0.5000        22
         hdx     0.5000    0.3529    0.4138        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.4667    0.5833    0.5185        12
         qtx     0.5833    0.7778    0.6667        45
         zxx     0.8000    0.6154    0.6957        39

    accuracy                         0.5514       185
   macro avg     0.5179    0.5043    0.4966       185
weighted avg     0.5550    0.5514    0.5397       185

micro f-score: 0.5513513513513514

========== Train Epoch 57 ==========
Loss: 0.009	Accuracy: 57.84%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5600    0.4516    0.5000        31
         cwx     0.4062    0.5909    0.4815        22
         hdx     0.5000    0.3529    0.4138        17
         mtx     0.4000    0.1053    0.1667        19
         nqx     0.4211    0.6667    0.5161        12
         qtx     0.7500    0.6667    0.7059        45
         zxx     0.6538    0.8718    0.7473        39

    accuracy                         0.5784       185
   macro avg     0.5273    0.5294    0.5045       185
weighted avg     0.5768    0.5784    0.5589       185

micro f-score: 0.5783783783783784

========== Train Epoch 58 ==========
Loss: 0.010	Accuracy: 58.92%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5152    0.5484    0.5312        31
         cwx     0.4615    0.5455    0.5000        22
         hdx     0.4667    0.4118    0.4375        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.6458    0.6889    0.6667        45
         zxx     0.7619    0.8205    0.7901        39

    accuracy                         0.5892       185
   macro avg     0.5378    0.5366    0.5297       185
weighted avg     0.5752    0.5892    0.5766       185

micro f-score: 0.5891891891891892

========== Train Epoch 59 ==========
Loss: 0.011	Accuracy: 60.00%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4865    0.5806    0.5294        31
         cwx     0.4688    0.6818    0.5556        22
         hdx     0.5000    0.5882    0.5405        17
         mtx     0.5000    0.1579    0.2400        19
         nqx     0.5000    0.5000    0.5000        12
         qtx     0.7778    0.6222    0.6914        45
         zxx     0.7381    0.7949    0.7654        39

    accuracy                         0.6000       185
   macro avg     0.5673    0.5608    0.5460       185
weighted avg     0.6118    0.6000    0.5911       185

micro f-score: 0.6

========== Train Epoch 60 ==========
Loss: 0.012	Accuracy: 61.08%	Cost 38s
              precision    recall  f1-score   support

         bzx     0.7000    0.4516    0.5490        31
         cwx     0.4524    0.8636    0.5938        22
         hdx     0.5000    0.2353    0.3200        17
         mtx     0.5000    0.1579    0.2400        19
         nqx     0.4545    0.4167    0.4348        12
         qtx     0.7333    0.7333    0.7333        45
         zxx     0.6604    0.8974    0.7609        39

    accuracy                         0.6108       185
   macro avg     0.5715    0.5366    0.5188       185
weighted avg     0.6155    0.6108    0.5836       185

micro f-score: 0.6108108108108108

========== Train Epoch 61 ==========
Loss: 0.010	Accuracy: 57.84%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5926    0.5161    0.5517        31
         cwx     0.3846    0.6818    0.4918        22
         hdx     0.4167    0.2941    0.3448        17
         mtx     0.6000    0.1579    0.2500        19
         nqx     0.4211    0.6667    0.5161        12
         qtx     0.7143    0.6667    0.6897        45
         zxx     0.7317    0.7692    0.7500        39

    accuracy                         0.5784       185
   macro avg     0.5516    0.5361    0.5134       185
weighted avg     0.6003    0.5784    0.5676       185

micro f-score: 0.5783783783783784

========== Train Epoch 62 ==========
Loss: 0.098	Accuracy: 40.54%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4444    0.2581    0.3265        31
         cwx     1.0000    0.0455    0.0870        22
         hdx     0.2759    0.4706    0.3478        17
         mtx     0.2727    0.1579    0.2000        19
         nqx     0.5000    0.1667    0.2500        12
         qtx     0.7308    0.4222    0.5352        45
         zxx     0.3542    0.8718    0.5037        39

    accuracy                         0.4054       185
   macro avg     0.5111    0.3418    0.3215       185
weighted avg     0.5316    0.4054    0.3701       185

micro f-score: 0.40540540540540543

========== Train Epoch 63 ==========
Loss: 0.806	Accuracy: 21.08%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.0000    0.0000    0.0000        22
         hdx     0.0000    0.0000    0.0000        17
         mtx     0.0476    0.0526    0.0500        19
         nqx     0.0000    0.0000    0.0000        12
         qtx     0.3636    0.0889    0.1429        45
         zxx     0.2237    0.8718    0.3560        39

    accuracy                         0.2108       185
   macro avg     0.0907    0.1448    0.0784       185
weighted avg     0.1405    0.2108    0.1149       185

micro f-score: 0.21081081081081082

========== Train Epoch 64 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.695	Accuracy: 40.00%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.6667    0.2581    0.3721        31
         cwx     0.3684    0.3182    0.3415        22
         hdx     0.1194    0.4706    0.1905        17
         mtx     0.2500    0.1579    0.1935        19
         nqx     0.6667    0.3333    0.4444        12
         qtx     0.6667    0.4000    0.5000        45
         zxx     0.6190    0.6667    0.6420        39

    accuracy                         0.4000       185
   macro avg     0.4796    0.3721    0.3834       185
weighted avg     0.5281    0.4000    0.4261       185

micro f-score: 0.4000000000000001

Finished training!!!

Min Loss = 0.009 in epoch 56;
Max Accuracy = 61.08% in epoch 59;
Total Cost 38 minutes

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (conv1_): Conv2d(3, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1_): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu_): ReLU(inplace=True)
  (maxpool_): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer2_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer3_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer4_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (avgpool_): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_): Linear(in_features=512, out_features=7, bias=True)
  (__fc__): Linear(in_features=1024, out_features=7, bias=True)
)

[0.32432432432432434, 0.40540540540540543, 0.41081081081081083, 0.4540540540540541, 0.32972972972972975, 0.4702702702702703, 0.3837837837837838, 0.5027027027027027, 0.4594594594594595, 0.5135135135135135, 0.35135135135135137, 0.5405405405405406, 0.4972972972972973, 0.5081081081081081, 0.41081081081081083, 0.42162162162162165, 0.4702702702702703, 0.4864864864864865, 0.5027027027027027, 0.5135135135135135, 0.43243243243243246, 0.5081081081081081, 0.4918918918918919, 0.4918918918918919, 0.4486486486486487, 0.5297297297297298, 0.5567567567567567, 0.5243243243243243, 0.5513513513513514, 0.5621621621621622, 0.5783783783783784, 0.5459459459459459, 0.572972972972973, 0.6, 0.5567567567567567, 0.5459459459459459, 0.5513513513513514, 0.5513513513513514, 0.572972972972973, 0.5945945945945946, 0.572972972972973, 0.5783783783783784, 0.5945945945945946, 0.5837837837837838, 0.5783783783783784, 0.572972972972973, 0.5675675675675675, 0.5567567567567567, 0.572972972972973, 0.572972972972973, 0.5783783783783784, 0.5513513513513514, 0.5783783783783784, 0.5621621621621622, 0.5945945945945946, 0.5513513513513514, 0.5783783783783784, 0.5891891891891892, 0.6, 0.6108108108108108, 0.5783783783783784, 0.40540540540540543, 0.21081081081081082, 0.4]
