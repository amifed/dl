dataset_path: /home/djy/dataset/dataset2
pretrained : False 
parallel: True

parallel segmentent dataset : /home/djy/dataset/ycrcb_hsv_dataset2
msg: cbam resnet34
using model: ResNet, resnet34
using device cuda:0
batch_size = 18
epochs = 64
loss_function = CrossEntropyLoss()
optimizer = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)


========== Train Epoch 1 ==========
Loss: 1.809	Accuracy: 34.05%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4000    0.0645    0.1111        31
         cwx     0.3077    0.1818    0.2286        22
         hdx     0.2500    0.0588    0.0952        17
         mtx     0.0000    0.0000    0.0000        19
         nqx     0.2500    0.0833    0.1250        12
         qtx     0.3305    0.8667    0.4785        45
         zxx     0.3902    0.4103    0.4000        39

    accuracy                         0.3405       185
   macro avg     0.2755    0.2379    0.2055       185
weighted avg     0.3055    0.3405    0.2634       185

micro f-score: 0.34054054054054056

========== Train Epoch 2 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.563	Accuracy: 33.51%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.0000    0.0000    0.0000        22
         hdx     0.5000    0.0588    0.1053        17
         mtx     0.5000    0.0526    0.0952        19
         nqx     0.3571    0.4167    0.3846        12
         qtx     0.4032    0.5556    0.4673        45
         zxx     0.2941    0.7692    0.4255        39

    accuracy                         0.3351       185
   macro avg     0.2935    0.2647    0.2111       185
weighted avg     0.2805    0.3351    0.2478       185

micro f-score: 0.33513513513513515

========== Train Epoch 3 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.390	Accuracy: 35.14%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.3000    0.5455    0.3871        22
         hdx     0.1290    0.2353    0.1667        17
         mtx     0.3000    0.1579    0.2069        19
         nqx     0.3077    0.3333    0.3200        12
         qtx     0.4231    0.7333    0.5366        45
         zxx     0.6923    0.2308    0.3462        39

    accuracy                         0.3514       185
   macro avg     0.3074    0.3194    0.2805       185
weighted avg     0.3472    0.3514    0.3068       185

micro f-score: 0.35135135135135137

========== Train Epoch 4 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 1.289	Accuracy: 46.49%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4583    0.3548    0.4000        31
         cwx     0.7143    0.2273    0.3448        22
         hdx     0.2000    0.1765    0.1875        17
         mtx     1.0000    0.1579    0.2727        19
         nqx     0.3077    0.6667    0.4211        12
         qtx     0.5273    0.6444    0.5800        45
         zxx     0.4909    0.6923    0.5745        39

    accuracy                         0.4649       185
   macro avg     0.5284    0.4171    0.3972       185
weighted avg     0.5345    0.4649    0.4428       185

micro f-score: 0.4648648648648649

========== Train Epoch 5 ==========
Loss: 1.003	Accuracy: 40.00%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.5263    0.3226    0.4000        31
         cwx     0.5714    0.1818    0.2759        22
         hdx     0.2308    0.1765    0.2000        17
         mtx     0.1045    0.3684    0.1628        19
         nqx     0.7500    0.2500    0.3750        12
         qtx     0.5192    0.6000    0.5567        45
         zxx     0.8696    0.5128    0.6452        39

    accuracy                         0.4000       185
   macro avg     0.5103    0.3446    0.3736       185
weighted avg     0.5463    0.4000    0.4307       185

micro f-score: 0.4000000000000001

========== Train Epoch 6 ==========
Loss: 0.835	Accuracy: 38.92%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.4815    0.5909    0.5306        22
         hdx     0.3333    0.0588    0.1000        17
         mtx     0.1818    0.2105    0.1951        19
         nqx     0.2292    0.9167    0.3667        12
         qtx     0.4750    0.8444    0.6080        45
         zxx     1.0000    0.1282    0.2273        39

    accuracy                         0.3892       185
   macro avg     0.3858    0.3928    0.2897       185
weighted avg     0.4478    0.3892    0.3119       185

micro f-score: 0.3891891891891892

========== Train Epoch 7 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.588	Accuracy: 51.89%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.3725    0.6129    0.4634        31
         cwx     0.5714    0.3636    0.4444        22
         hdx     0.3750    0.5294    0.4390        17
         mtx     0.3333    0.0526    0.0909        19
         nqx     0.5000    0.0833    0.1429        12
         qtx     0.6327    0.6889    0.6596        45
         zxx     0.6429    0.6923    0.6667        39

    accuracy                         0.5189       185
   macro avg     0.4897    0.4319    0.4153       185
weighted avg     0.5209    0.5189    0.4904       185

micro f-score: 0.518918918918919

========== Train Epoch 8 ==========
Loss: 0.351	Accuracy: 37.84%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.3231    0.6774    0.4375        31
         cwx     0.5714    0.3636    0.4444        22
         hdx     0.2143    0.5294    0.3051        17
         mtx     0.1818    0.2105    0.1951        19
         nqx     0.7500    0.2500    0.3750        12
         qtx     0.6286    0.4889    0.5500        45
         zxx     1.0000    0.0769    0.1429        39

    accuracy                         0.3784       185
   macro avg     0.5242    0.3710    0.3500       185
weighted avg     0.5728    0.3784    0.3625       185

micro f-score: 0.37837837837837834

========== Train Epoch 9 ==========
Loss: 0.231	Accuracy: 36.22%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.0000    0.0000    0.0000        22
         hdx     0.1800    0.5294    0.2687        17
         mtx     0.1912    0.6842    0.2989        19
         nqx     0.0000    0.0000    0.0000        12
         qtx     0.9231    0.2667    0.4138        45
         zxx     0.6226    0.8462    0.7174        39

    accuracy                         0.3622       185
   macro avg     0.2738    0.3323    0.2427       185
weighted avg     0.3920    0.3622    0.3073       185

micro f-score: 0.3621621621621622

========== Train Epoch 10 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.173	Accuracy: 57.84%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5714    0.1290    0.2105        31
         cwx     0.3864    0.7727    0.5152        22
         hdx     0.4545    0.2941    0.3571        17
         mtx     1.0000    0.1053    0.1905        19
         nqx     0.7143    0.4167    0.5263        12
         qtx     0.7200    0.8000    0.7579        45
         zxx     0.5938    0.9744    0.7379        39

    accuracy                         0.5784       185
   macro avg     0.6343    0.4989    0.4708       185
weighted avg     0.6328    0.5784    0.5230       185

micro f-score: 0.5783783783783784

========== Train Epoch 11 ==========
Loss: 0.102	Accuracy: 52.97%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4062    0.4194    0.4127        31
         cwx     0.3750    0.4091    0.3913        22
         hdx     0.5000    0.2941    0.3704        17
         mtx     0.2857    0.4211    0.3404        19
         nqx     0.4375    0.5833    0.5000        12
         qtx     0.6596    0.6889    0.6739        45
         zxx     0.8929    0.6410    0.7463        39

    accuracy                         0.5297       185
   macro avg     0.5081    0.4938    0.4907       185
weighted avg     0.5650    0.5297    0.5384       185

micro f-score: 0.5297297297297298

========== Train Epoch 12 ==========
Loss: 0.082	Accuracy: 53.51%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4000    0.4516    0.4242        31
         cwx     0.5500    0.5000    0.5238        22
         hdx     0.3103    0.5294    0.3913        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.6000    0.8000    0.6857        45
         zxx     0.9500    0.4872    0.6441        39

    accuracy                         0.5351       185
   macro avg     0.5320    0.5013    0.4931       185
weighted avg     0.5806    0.5351    0.5311       185

micro f-score: 0.5351351351351351

========== Train Epoch 13 ==========
Loss: 0.047	Accuracy: 56.22%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5152    0.5484    0.5312        31
         cwx     0.6000    0.2727    0.3750        22
         hdx     0.3750    0.3529    0.3636        17
         mtx     0.3636    0.2105    0.2667        19
         nqx     0.5000    0.2500    0.3333        12
         qtx     0.5405    0.8889    0.6723        45
         zxx     0.8000    0.7179    0.7568        39

    accuracy                         0.5622       185
   macro avg     0.5278    0.4631    0.4713       185
weighted avg     0.5620    0.5622    0.5391       185

micro f-score: 0.5621621621621622

========== Train Epoch 14 ==========
Loss: 0.043	Accuracy: 57.84%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4565    0.6774    0.5455        31
         cwx     0.6429    0.4091    0.5000        22
         hdx     0.3333    0.2941    0.3125        17
         mtx     0.2500    0.2105    0.2286        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.8049    0.7333    0.7674        45
         zxx     0.6977    0.7692    0.7317        39

    accuracy                         0.5784       185
   macro avg     0.5265    0.5015    0.5057       185
weighted avg     0.5845    0.5784    0.5735       185

micro f-score: 0.5783783783783784

========== Train Epoch 15 ==========
Loss: 0.035	Accuracy: 60.00%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.4828    0.4516    0.4667        31
         cwx     0.8462    0.5000    0.6286        22
         hdx     0.4400    0.6471    0.5238        17
         mtx     0.7143    0.2632    0.3846        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.8333    0.6667    0.7407        45
         zxx     0.5323    0.8462    0.6535        39

    accuracy                         0.6000       185
   macro avg     0.6268    0.5654    0.5654       185
weighted avg     0.6451    0.6000    0.5948       185

micro f-score: 0.6

========== Train Epoch 16 ==========
Loss: 0.033	Accuracy: 58.92%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5185    0.4516    0.4828        31
         cwx     0.6154    0.7273    0.6667        22
         hdx     0.2500    0.2353    0.2424        17
         mtx     0.1667    0.1053    0.1290        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.7556    0.7556    0.7556        45
         zxx     0.7174    0.8462    0.7765        39

    accuracy                         0.5892       185
   macro avg     0.4979    0.5173    0.5047       185
weighted avg     0.5651    0.5892    0.5743       185

micro f-score: 0.5891891891891892

========== Train Epoch 17 ==========
Loss: 0.023	Accuracy: 61.08%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4872    0.6129    0.5429        31
         cwx     0.6190    0.5909    0.6047        22
         hdx     0.4375    0.4118    0.4242        17
         mtx     0.3750    0.1579    0.2222        19
         nqx     0.4667    0.5833    0.5185        12
         qtx     0.7805    0.7111    0.7442        45
         zxx     0.7111    0.8205    0.7619        39

    accuracy                         0.6108       185
   macro avg     0.5539    0.5555    0.5455       185
weighted avg     0.6040    0.6108    0.5999       185

micro f-score: 0.6108108108108108

========== Train Epoch 18 ==========
Loss: 0.024	Accuracy: 58.38%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5000    0.4516    0.4746        31
         cwx     0.6190    0.5909    0.6047        22
         hdx     0.3750    0.5294    0.4390        17
         mtx     0.2400    0.3158    0.2727        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.8095    0.7556    0.7816        45
         zxx     0.8125    0.6667    0.7324        39

    accuracy                         0.5838       185
   macro avg     0.5454    0.5443    0.5407       185
weighted avg     0.6146    0.5838    0.5954       185

micro f-score: 0.5837837837837838

========== Train Epoch 19 ==========
Loss: 0.025	Accuracy: 56.76%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4375    0.4516    0.4444        31
         cwx     0.6667    0.4545    0.5405        22
         hdx     0.3571    0.2941    0.3226        17
         mtx     0.3000    0.1579    0.2069        19
         nqx     0.5455    0.5000    0.5217        12
         qtx     0.8049    0.7333    0.7674        45
         zxx     0.5484    0.8718    0.6733        39

    accuracy                         0.5676       185
   macro avg     0.5229    0.4948    0.4967       185
weighted avg     0.5630    0.5676    0.5521       185

micro f-score: 0.5675675675675675

========== Train Epoch 20 ==========
Loss: 0.020	Accuracy: 58.38%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4583    0.3548    0.4000        31
         cwx     0.6250    0.6818    0.6522        22
         hdx     0.7143    0.2941    0.4167        17
         mtx     0.3077    0.2105    0.2500        19
         nqx     0.5455    0.5000    0.5217        12
         qtx     0.6032    0.8444    0.7037        45
         zxx     0.6744    0.7436    0.7073        39

    accuracy                         0.5838       185
   macro avg     0.5612    0.5185    0.5217       185
weighted avg     0.5726    0.5838    0.5627       185

micro f-score: 0.5837837837837838

========== Train Epoch 21 ==========
Loss: 0.027	Accuracy: 52.43%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3542    0.5484    0.4304        31
         cwx     0.5789    0.5000    0.5366        22
         hdx     0.8000    0.2353    0.3636        17
         mtx     0.2581    0.4211    0.3200        19
         nqx     0.4000    0.5000    0.4444        12
         qtx     0.9259    0.5556    0.6944        45
         zxx     0.6500    0.6667    0.6582        39

    accuracy                         0.5243       185
   macro avg     0.5667    0.4896    0.4925       185
weighted avg     0.6164    0.5243    0.5387       185

micro f-score: 0.5243243243243243

========== Train Epoch 22 ==========
Loss: 0.031	Accuracy: 55.68%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5625    0.2903    0.3830        31
         cwx     0.5200    0.5909    0.5532        22
         hdx     1.0000    0.1176    0.2105        17
         mtx     0.3636    0.2105    0.2667        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.5256    0.9111    0.6667        45
         zxx     0.6744    0.7436    0.7073        39

    accuracy                         0.5568       185
   macro avg     0.5923    0.4687    0.4631       185
weighted avg     0.5878    0.5568    0.5174       185

micro f-score: 0.5567567567567567

========== Train Epoch 23 ==========
Loss: 0.029	Accuracy: 60.00%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.6250    0.3226    0.4255        31
         cwx     0.6667    0.6364    0.6512        22
         hdx     0.5000    0.4118    0.4516        17
         mtx     0.5000    0.2105    0.2963        19
         nqx     0.4667    0.5833    0.5185        12
         qtx     0.5256    0.9111    0.6667        45
         zxx     0.8485    0.7179    0.7778        39

    accuracy                         0.6000       185
   macro avg     0.5904    0.5419    0.5411       185
weighted avg     0.6183    0.6000    0.5804       185

micro f-score: 0.6

========== Train Epoch 24 ==========
Loss: 0.017	Accuracy: 63.78%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5000    0.6129    0.5507        31
         cwx     0.5172    0.6818    0.5882        22
         hdx     0.5625    0.5294    0.5455        17
         mtx     0.5000    0.2632    0.3448        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.7857    0.7333    0.7586        45
         zxx     0.8378    0.7949    0.8158        39

    accuracy                         0.6378       185
   macro avg     0.5950    0.5879    0.5834       185
weighted avg     0.6460    0.6378    0.6354       185

micro f-score: 0.6378378378378379

========== Train Epoch 25 ==========
Loss: 0.020	Accuracy: 64.32%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4545    0.4839    0.4687        31
         cwx     0.6957    0.7273    0.7111        22
         hdx     0.5455    0.3529    0.4286        17
         mtx     0.6000    0.3158    0.4138        19
         nqx     0.4706    0.6667    0.5517        12
         qtx     0.7556    0.7556    0.7556        45
         zxx     0.7391    0.8718    0.8000        39

    accuracy                         0.6432       185
   macro avg     0.6087    0.5963    0.5899       185
weighted avg     0.6408    0.6432    0.6332       185

micro f-score: 0.6432432432432432

========== Train Epoch 26 ==========
Loss: 0.016	Accuracy: 60.54%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5000    0.2258    0.3111        31
         cwx     0.7000    0.6364    0.6667        22
         hdx     0.5556    0.2941    0.3846        17
         mtx     0.5000    0.2632    0.3448        19
         nqx     0.5714    0.6667    0.6154        12
         qtx     0.6792    0.8000    0.7347        45
         zxx     0.5692    0.9487    0.7115        39

    accuracy                         0.6054       185
   macro avg     0.5822    0.5478    0.5384       185
weighted avg     0.5917    0.6054    0.5708       185

micro f-score: 0.6054054054054054

========== Train Epoch 27 ==========
Loss: 0.017	Accuracy: 67.57%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4839    0.4839    0.4839        31
         cwx     0.6957    0.7273    0.7111        22
         hdx     0.6250    0.5882    0.6061        17
         mtx     0.6667    0.2105    0.3200        19
         nqx     0.6667    0.6667    0.6667        12
         qtx     0.7872    0.8222    0.8043        45
         zxx     0.7000    0.8974    0.7865        39

    accuracy                         0.6757       185
   macro avg     0.6607    0.6280    0.6255       185
weighted avg     0.6720    0.6757    0.6589       185

micro f-score: 0.6756756756756757

========== Train Epoch 28 ==========
Loss: 0.025	Accuracy: 58.92%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4667    0.4516    0.4590        31
         cwx     0.7500    0.5455    0.6316        22
         hdx     0.6250    0.2941    0.4000        17
         mtx     0.6667    0.2105    0.3200        19
         nqx     0.6667    0.6667    0.6667        12
         qtx     0.4824    0.9111    0.6308        45
         zxx     0.8929    0.6410    0.7463        39

    accuracy                         0.5892       185
   macro avg     0.6500    0.5315    0.5506       185
weighted avg     0.6421    0.5892    0.5756       185

micro f-score: 0.5891891891891892

========== Train Epoch 29 ==========
Loss: 0.019	Accuracy: 64.32%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4688    0.4839    0.4762        31
         cwx     0.5172    0.6818    0.5882        22
         hdx     0.5882    0.5882    0.5882        17
         mtx     0.5000    0.1053    0.1739        19
         nqx     0.5333    0.6667    0.5926        12
         qtx     0.8333    0.7778    0.8046        45
         zxx     0.7391    0.8718    0.8000        39

    accuracy                         0.6432       185
   macro avg     0.5971    0.5965    0.5748       185
weighted avg     0.6386    0.6432    0.6245       185

micro f-score: 0.6432432432432432

========== Train Epoch 30 ==========
Loss: 0.014	Accuracy: 64.32%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.6111    0.3548    0.4490        31
         cwx     0.6000    0.6818    0.6383        22
         hdx     0.6154    0.4706    0.5333        17
         mtx     0.7143    0.2632    0.3846        19
         nqx     0.5000    0.8333    0.6250        12
         qtx     0.8049    0.7333    0.7674        45
         zxx     0.6066    0.9487    0.7400        39

    accuracy                         0.6432       185
   macro avg     0.6360    0.6123    0.5911       185
weighted avg     0.6597    0.6432    0.6229       185

micro f-score: 0.6432432432432432

========== Train Epoch 31 ==========
Loss: 0.015	Accuracy: 63.24%	Cost 34s
              precision    recall  f1-score   support

         bzx     0.5000    0.4516    0.4746        31
         cwx     0.6818    0.6818    0.6818        22
         hdx     0.5000    0.2353    0.3200        17
         mtx     0.5000    0.3158    0.3871        19
         nqx     0.5714    0.6667    0.6154        12
         qtx     0.7400    0.8222    0.7789        45
         zxx     0.6471    0.8462    0.7333        39

    accuracy                         0.6324       185
   macro avg     0.5915    0.5742    0.5702       185
weighted avg     0.6156    0.6324    0.6138       185

micro f-score: 0.6324324324324324

========== Train Epoch 32 ==========
Loss: 0.017	Accuracy: 64.32%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5263    0.3226    0.4000        31
         cwx     0.7500    0.6818    0.7143        22
         hdx     0.4545    0.5882    0.5128        17
         mtx     0.5000    0.2105    0.2963        19
         nqx     0.6364    0.5833    0.6087        12
         qtx     0.8750    0.7778    0.8235        45
         zxx     0.5846    0.9744    0.7308        39

    accuracy                         0.6432       185
   macro avg     0.6181    0.5912    0.5838       185
weighted avg     0.6479    0.6432    0.6234       185

micro f-score: 0.6432432432432432

========== Train Epoch 33 ==========
Loss: 0.016	Accuracy: 61.08%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4783    0.3548    0.4074        31
         cwx     0.6522    0.6818    0.6667        22
         hdx     0.5833    0.4118    0.4828        17
         mtx     0.4286    0.1579    0.2308        19
         nqx     0.4500    0.7500    0.5625        12
         qtx     0.8095    0.7556    0.7816        45
         zxx     0.5862    0.8718    0.7010        39

    accuracy                         0.6108       185
   macro avg     0.5697    0.5691    0.5475       185
weighted avg     0.6050    0.6108    0.5900       185

micro f-score: 0.6108108108108108

========== Train Epoch 34 ==========
Loss: 0.014	Accuracy: 62.70%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5152    0.5484    0.5312        31
         cwx     0.5556    0.6818    0.6122        22
         hdx     0.4444    0.4706    0.4571        17
         mtx     0.3077    0.2105    0.2500        19
         nqx     0.5625    0.7500    0.6429        12
         qtx     0.8293    0.7556    0.7907        45
         zxx     0.7838    0.7436    0.7632        39

    accuracy                         0.6270       185
   macro avg     0.5712    0.5944    0.5782       185
weighted avg     0.6283    0.6270    0.6244       185

micro f-score: 0.6270270270270271

========== Train Epoch 35 ==========
Loss: 0.013	Accuracy: 63.24%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5000    0.3871    0.4364        31
         cwx     0.7000    0.6364    0.6667        22
         hdx     0.4286    0.5294    0.4737        17
         mtx     0.5714    0.2105    0.3077        19
         nqx     0.5455    0.5000    0.5217        12
         qtx     0.8750    0.7778    0.8235        45
         zxx     0.5968    0.9487    0.7327        39

    accuracy                         0.6324       185
   macro avg     0.6025    0.5700    0.5660       185
weighted avg     0.6391    0.6324    0.6161       185

micro f-score: 0.6324324324324324

========== Train Epoch 36 ==========
Loss: 0.011	Accuracy: 64.86%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5152    0.5484    0.5312        31
         cwx     0.7500    0.6818    0.7143        22
         hdx     0.3636    0.4706    0.4103        17
         mtx     0.5000    0.2632    0.3448        19
         nqx     0.5333    0.6667    0.5926        12
         qtx     0.9024    0.8222    0.8605        45
         zxx     0.6818    0.7692    0.7229        39

    accuracy                         0.6486       185
   macro avg     0.6066    0.6032    0.5967       185
weighted avg     0.6581    0.6486    0.6472       185

micro f-score: 0.6486486486486487

========== Train Epoch 37 ==========
Loss: 0.013	Accuracy: 63.78%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4524    0.6129    0.5205        31
         cwx     0.6522    0.6818    0.6667        22
         hdx     0.7143    0.2941    0.4167        17
         mtx     0.5000    0.2632    0.3448        19
         nqx     0.5833    0.5833    0.5833        12
         qtx     0.8182    0.8000    0.8090        45
         zxx     0.6596    0.7949    0.7209        39

    accuracy                         0.6378       185
   macro avg     0.6257    0.5757    0.5803       185
weighted avg     0.6462    0.6378    0.6268       185

micro f-score: 0.6378378378378379

========== Train Epoch 38 ==========
Loss: 0.013	Accuracy: 62.16%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5417    0.4194    0.4727        31
         cwx     0.6250    0.6818    0.6522        22
         hdx     0.3600    0.5294    0.4286        17
         mtx     0.4444    0.2105    0.2857        19
         nqx     0.4737    0.7500    0.5806        12
         qtx     0.9118    0.6889    0.7848        45
         zxx     0.6800    0.8718    0.7640        39

    accuracy                         0.6216       185
   macro avg     0.5767    0.5931    0.5670       185
weighted avg     0.6397    0.6216    0.6151       185

micro f-score: 0.6216216216216216

========== Train Epoch 39 ==========
Loss: 0.012	Accuracy: 61.08%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5294    0.2903    0.3750        31
         cwx     0.6667    0.6364    0.6512        22
         hdx     0.5294    0.5294    0.5294        17
         mtx     0.6667    0.2105    0.3200        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.8462    0.7333    0.7857        45
         zxx     0.5139    0.9487    0.6667        39

    accuracy                         0.6108       185
   macro avg     0.6130    0.5617    0.5554       185
weighted avg     0.6342    0.6108    0.5898       185

micro f-score: 0.6108108108108108

========== Train Epoch 40 ==========
Loss: 0.030	Accuracy: 42.16%	Cost 36s
              precision    recall  f1-score   support

         bzx     1.0000    0.0645    0.1212        31
         cwx     0.2687    0.8182    0.4045        22
         hdx     0.3333    0.1176    0.1739        17
         mtx     0.1333    0.1053    0.1176        19
         nqx     0.5000    0.2500    0.3333        12
         qtx     0.4933    0.8222    0.6167        45
         zxx     1.0000    0.3590    0.5283        39

    accuracy                         0.4216       185
   macro avg     0.5327    0.3624    0.3279       185
weighted avg     0.6071    0.4216    0.3795       185

micro f-score: 0.42162162162162165

========== Train Epoch 41 ==========
Loss: 0.481	Accuracy: 27.57%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4444    0.3871    0.4138        31
         cwx     0.5000    0.0455    0.0833        22
         hdx     0.1481    0.2353    0.1818        17
         mtx     0.1500    0.4737    0.2278        19
         nqx     0.2941    0.4167    0.3448        12
         qtx     0.0000    0.0000    0.0000        45
         zxx     0.3846    0.5128    0.4396        39

    accuracy                         0.2757       185
   macro avg     0.2745    0.2959    0.2416       185
weighted avg     0.2631    0.2757    0.2344       185

micro f-score: 0.2756756756756757

========== Train Epoch 42 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.916	Accuracy: 28.11%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.0000    0.0000    0.0000        31
         cwx     0.0000    0.0000    0.0000        22
         hdx     0.1176    0.8235    0.2059        17
         mtx     0.0000    0.0000    0.0000        19
         nqx     0.0000    0.0000    0.0000        12
         qtx     0.5000    0.2889    0.3662        45
         zxx     0.6250    0.6410    0.6329        39

    accuracy                         0.2811       185
   macro avg     0.1775    0.2505    0.1721       185
weighted avg     0.2642    0.2811    0.2414       185

micro f-score: 0.2810810810810811

========== Train Epoch 43 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.328	Accuracy: 43.24%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4667    0.2258    0.3043        31
         cwx     0.8000    0.1818    0.2963        22
         hdx     0.2500    0.3529    0.2927        17
         mtx     0.2812    0.4737    0.3529        19
         nqx     0.1864    0.9167    0.3099        12
         qtx     0.8889    0.5333    0.6667        45
         zxx     0.8261    0.4872    0.6129        39

    accuracy                         0.4324       185
   macro avg     0.5285    0.4531    0.4051       185
weighted avg     0.6276    0.4324    0.4608       185

micro f-score: 0.43243243243243246

========== Train Epoch 44 ==========
Loss: 0.126	Accuracy: 50.27%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3810    0.5161    0.4384        31
         cwx     0.3409    0.6818    0.4545        22
         hdx     0.4000    0.3529    0.3750        17
         mtx     0.0000    0.0000    0.0000        19
         nqx     0.7500    0.2500    0.3750        12
         qtx     0.6226    0.7333    0.6735        45
         zxx     0.7407    0.5128    0.6061        39

    accuracy                         0.5027       185
   macro avg     0.4622    0.4353    0.4175       185
weighted avg     0.4974    0.5027    0.4779       185

micro f-score: 0.5027027027027027

========== Train Epoch 45 ==========
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/djy/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Loss: 0.089	Accuracy: 55.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4375    0.2258    0.2979        31
         cwx     0.7143    0.4545    0.5556        22
         hdx     0.3182    0.4118    0.3590        17
         mtx     0.4118    0.3684    0.3889        19
         nqx     0.4444    0.6667    0.5333        12
         qtx     0.6735    0.7333    0.7021        45
         zxx     0.6122    0.7692    0.6818        39

    accuracy                         0.5514       185
   macro avg     0.5160    0.5185    0.5027       185
weighted avg     0.5515    0.5514    0.5380       185

micro f-score: 0.5513513513513514

========== Train Epoch 46 ==========
Loss: 0.043	Accuracy: 54.59%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.3871    0.3871    0.3871        31
         cwx     0.6000    0.6818    0.6383        22
         hdx     0.4167    0.2941    0.3448        17
         mtx     0.2857    0.1053    0.1538        19
         nqx     0.5333    0.6667    0.5926        12
         qtx     0.5538    0.8000    0.6545        45
         zxx     0.7667    0.5897    0.6667        39

    accuracy                         0.5459       185
   macro avg     0.5062    0.5035    0.4911       185
weighted avg     0.5348    0.5459    0.5265       185

micro f-score: 0.5459459459459459

========== Train Epoch 47 ==========
Loss: 0.037	Accuracy: 57.30%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.4359    0.5484    0.4857        31
         cwx     0.5200    0.5909    0.5532        22
         hdx     0.3810    0.4706    0.4211        17
         mtx     0.4444    0.2105    0.2857        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.8788    0.6444    0.7436        45
         zxx     0.6250    0.7692    0.6897        39

    accuracy                         0.5730       185
   macro avg     0.5407    0.5215    0.5191       185
weighted avg     0.5935    0.5730    0.5710       185

micro f-score: 0.572972972972973

========== Train Epoch 48 ==========
Loss: 0.021	Accuracy: 55.14%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5714    0.1290    0.2105        31
         cwx     0.4571    0.7273    0.5614        22
         hdx     0.3810    0.4706    0.4211        17
         mtx     0.2857    0.2105    0.2424        19
         nqx     0.3500    0.5833    0.4375        12
         qtx     0.6939    0.7556    0.7234        45
         zxx     0.7436    0.7436    0.7436        39

    accuracy                         0.5514       185
   macro avg     0.4975    0.5171    0.4771       185
weighted avg     0.5627    0.5514    0.5267       185

micro f-score: 0.5513513513513514

========== Train Epoch 49 ==========
Loss: 0.030	Accuracy: 57.84%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.4048    0.5484    0.4658        31
         cwx     0.6842    0.5909    0.6341        22
         hdx     0.5000    0.3529    0.4138        17
         mtx     0.2500    0.2105    0.2286        19
         nqx     0.6250    0.4167    0.5000        12
         qtx     0.8049    0.7333    0.7674        45
         zxx     0.6170    0.7436    0.6744        39

    accuracy                         0.5784       185
   macro avg     0.5551    0.5138    0.5263       185
weighted avg     0.5872    0.5784    0.5762       185

micro f-score: 0.5783783783783784

========== Train Epoch 50 ==========
Loss: 0.020	Accuracy: 55.68%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5714    0.2581    0.3556        31
         cwx     0.5385    0.6364    0.5833        22
         hdx     0.3103    0.5294    0.3913        17
         mtx     0.2222    0.1053    0.1429        19
         nqx     0.5000    0.5000    0.5000        12
         qtx     0.8056    0.6444    0.7160        45
         zxx     0.5932    0.8974    0.7143        39

    accuracy                         0.5568       185
   macro avg     0.5059    0.5101    0.4862       185
weighted avg     0.5646    0.5568    0.5368       185

micro f-score: 0.5567567567567567

========== Train Epoch 51 ==========
Loss: 0.021	Accuracy: 56.22%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5161    0.5161    0.5161        31
         cwx     0.5000    0.6818    0.5769        22
         hdx     0.3889    0.4118    0.4000        17
         mtx     0.1818    0.1053    0.1333        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.7750    0.6889    0.7294        45
         zxx     0.6222    0.7179    0.6667        39

    accuracy                         0.5622       185
   macro avg     0.4977    0.5055    0.4967       185
weighted avg     0.5525    0.5622    0.5530       185

micro f-score: 0.5621621621621622

========== Train Epoch 52 ==========
Loss: 0.020	Accuracy: 55.68%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5000    0.4194    0.4561        31
         cwx     0.5172    0.6818    0.5882        22
         hdx     0.3043    0.4118    0.3500        17
         mtx     0.1765    0.1579    0.1667        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.8333    0.6667    0.7407        45
         zxx     0.7073    0.7436    0.7250        39

    accuracy                         0.5568       185
   macro avg     0.5000    0.5116    0.5010       185
weighted avg     0.5731    0.5568    0.5598       185

micro f-score: 0.5567567567567567

========== Train Epoch 53 ==========
Loss: 0.017	Accuracy: 58.38%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5600    0.4516    0.5000        31
         cwx     0.5357    0.6818    0.6000        22
         hdx     0.3636    0.4706    0.4103        17
         mtx     0.1765    0.1579    0.1667        19
         nqx     0.5000    0.5000    0.5000        12
         qtx     0.8205    0.7111    0.7619        45
         zxx     0.7143    0.7692    0.7407        39

    accuracy                         0.5838       185
   macro avg     0.5244    0.5346    0.5257       185
weighted avg     0.5917    0.5838    0.5839       185

micro f-score: 0.5837837837837838

========== Train Epoch 54 ==========
Loss: 0.014	Accuracy: 60.00%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5833    0.4516    0.5091        31
         cwx     0.5385    0.6364    0.5833        22
         hdx     0.3846    0.5882    0.4651        17
         mtx     0.3333    0.1579    0.2143        19
         nqx     0.5385    0.5833    0.5600        12
         qtx     0.9333    0.6222    0.7467        45
         zxx     0.6140    0.8974    0.7292        39

    accuracy                         0.6000       185
   macro avg     0.5608    0.5624    0.5440       185
weighted avg     0.6228    0.6000    0.5911       185

micro f-score: 0.6

========== Train Epoch 55 ==========
Loss: 0.014	Accuracy: 59.46%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5714    0.3871    0.4615        31
         cwx     0.5172    0.6818    0.5882        22
         hdx     0.4000    0.4706    0.4324        17
         mtx     0.2308    0.1579    0.1875        19
         nqx     0.5455    0.5000    0.5217        12
         qtx     0.8857    0.6889    0.7750        45
         zxx     0.6250    0.8974    0.7368        39

    accuracy                         0.5946       185
   macro avg     0.5394    0.5405    0.5290       185
weighted avg     0.6003    0.5946    0.5840       185

micro f-score: 0.5945945945945946

========== Train Epoch 56 ==========
Loss: 0.011	Accuracy: 61.62%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5312    0.5484    0.5397        31
         cwx     0.5556    0.6818    0.6122        22
         hdx     0.3889    0.4118    0.4000        17
         mtx     0.3333    0.2105    0.2581        19
         nqx     0.5556    0.4167    0.4762        12
         qtx     0.8462    0.7333    0.7857        45
         zxx     0.6875    0.8462    0.7586        39

    accuracy                         0.6162       185
   macro avg     0.5569    0.5498    0.5472       185
weighted avg     0.6118    0.6162    0.6084       185

micro f-score: 0.6162162162162163

========== Train Epoch 57 ==========
Loss: 0.012	Accuracy: 58.92%	Cost 37s
              precision    recall  f1-score   support

         bzx     0.5000    0.3871    0.4364        31
         cwx     0.5600    0.6364    0.5957        22
         hdx     0.3750    0.5294    0.4390        17
         mtx     0.3333    0.2105    0.2581        19
         nqx     0.5000    0.4167    0.4545        12
         qtx     0.9091    0.6667    0.7692        45
         zxx     0.6140    0.8974    0.7292        39

    accuracy                         0.5892       185
   macro avg     0.5416    0.5349    0.5260       185
weighted avg     0.6021    0.5892    0.5811       185

micro f-score: 0.5891891891891892

========== Train Epoch 58 ==========
Loss: 0.012	Accuracy: 60.00%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4857    0.5484    0.5152        31
         cwx     0.5357    0.6818    0.6000        22
         hdx     0.4000    0.4706    0.4324        17
         mtx     0.2500    0.1579    0.1935        19
         nqx     0.5000    0.5000    0.5000        12
         qtx     0.8205    0.7111    0.7619        45
         zxx     0.7692    0.7692    0.7692        39

    accuracy                         0.6000       185
   macro avg     0.5373    0.5484    0.5389       185
weighted avg     0.6017    0.6000    0.5972       185

micro f-score: 0.6

========== Train Epoch 59 ==========
Loss: 0.016	Accuracy: 58.38%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4667    0.4516    0.4590        31
         cwx     0.5357    0.6818    0.6000        22
         hdx     0.4118    0.4118    0.4118        17
         mtx     0.2727    0.1579    0.2000        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.9091    0.6667    0.7692        45
         zxx     0.6226    0.8462    0.7174        39

    accuracy                         0.5838       185
   macro avg     0.5257    0.5308    0.5196       185
weighted avg     0.5901    0.5838    0.5761       185

micro f-score: 0.5837837837837838

========== Train Epoch 60 ==========
Loss: 0.013	Accuracy: 59.46%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5385    0.4516    0.4912        31
         cwx     0.5357    0.6818    0.6000        22
         hdx     0.4000    0.5882    0.4762        17
         mtx     0.2222    0.1053    0.1429        19
         nqx     0.4167    0.4167    0.4167        12
         qtx     0.8611    0.6889    0.7654        45
         zxx     0.6735    0.8462    0.7500        39

    accuracy                         0.5946       185
   macro avg     0.5211    0.5398    0.5203       185
weighted avg     0.5920    0.5946    0.5834       185

micro f-score: 0.5945945945945946

========== Train Epoch 61 ==========
Loss: 0.011	Accuracy: 58.38%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4615    0.3871    0.4211        31
         cwx     0.5769    0.6818    0.6250        22
         hdx     0.4000    0.3529    0.3750        17
         mtx     0.3000    0.1579    0.2069        19
         nqx     0.4167    0.4167    0.4167        12
         qtx     0.7907    0.7556    0.7727        45
         zxx     0.6226    0.8462    0.7174        39

    accuracy                         0.5838       185
   macro avg     0.5098    0.5140    0.5050       185
weighted avg     0.5641    0.5838    0.5668       185

micro f-score: 0.5837837837837838

========== Train Epoch 62 ==========
Loss: 0.013	Accuracy: 57.84%	Cost 35s
              precision    recall  f1-score   support

         bzx     0.5455    0.3871    0.4528        31
         cwx     0.5385    0.6364    0.5833        22
         hdx     0.3333    0.4118    0.3684        17
         mtx     0.2500    0.1053    0.1481        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.8611    0.6889    0.7654        45
         zxx     0.5932    0.8974    0.7143        39

    accuracy                         0.5784       185
   macro avg     0.5119    0.5181    0.5018       185
weighted avg     0.5762    0.5784    0.5622       185

micro f-score: 0.5783783783783784

========== Train Epoch 63 ==========
Loss: 0.010	Accuracy: 57.84%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.4828    0.4516    0.4667        31
         cwx     0.5172    0.6818    0.5882        22
         hdx     0.3529    0.3529    0.3529        17
         mtx     0.2222    0.1053    0.1429        19
         nqx     0.4615    0.5000    0.4800        12
         qtx     0.8421    0.7111    0.7711        45
         zxx     0.6400    0.8205    0.7191        39

    accuracy                         0.5784       185
   macro avg     0.5027    0.5176    0.5030       185
weighted avg     0.5674    0.5784    0.5655       185

micro f-score: 0.5783783783783784

========== Train Epoch 64 ==========
Loss: 0.010	Accuracy: 61.62%	Cost 36s
              precision    recall  f1-score   support

         bzx     0.5172    0.4839    0.5000        31
         cwx     0.5517    0.7273    0.6275        22
         hdx     0.3810    0.4706    0.4211        17
         mtx     0.3846    0.2632    0.3125        19
         nqx     0.5333    0.6667    0.5926        12
         qtx     0.8611    0.6889    0.7654        45
         zxx     0.7381    0.7949    0.7654        39

    accuracy                         0.6162       185
   macro avg     0.5667    0.5850    0.5692       185
weighted avg     0.6264    0.6162    0.6152       185

micro f-score: 0.6162162162162163

Finished training!!!

Min Loss = 0.010 in epoch 62;
Max Accuracy = 67.57% in epoch 26;
Total Cost 38 minutes

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (conv1_): Conv2d(3, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1_): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu_): ReLU(inplace=True)
  (maxpool_): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=64, out_features=4, bias=True)
            (2): ReLU()
            (3): Linear(in_features=4, out_features=64, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer2_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): ReLU()
            (3): Linear(in_features=8, out_features=128, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer3_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (3): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (4): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (5): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=256, out_features=16, bias=True)
            (2): ReLU()
            (3): Linear(in_features=16, out_features=256, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (layer4_): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
    (2): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (cbam): CBAM(
        (ChannelGate): ChannelGate(
          (mlp): Sequential(
            (0): Flatten()
            (1): Linear(in_features=512, out_features=32, bias=True)
            (2): ReLU()
            (3): Linear(in_features=32, out_features=512, bias=True)
          )
        )
        (SpatialGate): SpatialGate(
          (compress): ChannelPool()
          (spatial): BasicConv(
            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (avgpool_): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_): Linear(in_features=512, out_features=7, bias=True)
  (__fc__): Linear(in_features=1024, out_features=7, bias=True)
)

[0.34054054054054056, 0.33513513513513515, 0.35135135135135137, 0.4648648648648649, 0.4, 0.3891891891891892, 0.518918918918919, 0.3783783783783784, 0.3621621621621622, 0.5783783783783784, 0.5297297297297298, 0.5351351351351351, 0.5621621621621622, 0.5783783783783784, 0.6, 0.5891891891891892, 0.6108108108108108, 0.5837837837837838, 0.5675675675675675, 0.5837837837837838, 0.5243243243243243, 0.5567567567567567, 0.6, 0.6378378378378379, 0.6432432432432432, 0.6054054054054054, 0.6756756756756757, 0.5891891891891892, 0.6432432432432432, 0.6432432432432432, 0.6324324324324324, 0.6432432432432432, 0.6108108108108108, 0.6270270270270271, 0.6324324324324324, 0.6486486486486487, 0.6378378378378379, 0.6216216216216216, 0.6108108108108108, 0.42162162162162165, 0.2756756756756757, 0.2810810810810811, 0.43243243243243246, 0.5027027027027027, 0.5513513513513514, 0.5459459459459459, 0.572972972972973, 0.5513513513513514, 0.5783783783783784, 0.5567567567567567, 0.5621621621621622, 0.5567567567567567, 0.5837837837837838, 0.6, 0.5945945945945946, 0.6162162162162163, 0.5891891891891892, 0.6, 0.5837837837837838, 0.5945945945945946, 0.5837837837837838, 0.5783783783783784, 0.5783783783783784, 0.6162162162162163]
